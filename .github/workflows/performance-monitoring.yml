name: Performance Monitoring

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  schedule:
    # Run performance tests daily at 2 AM UTC
    - cron: '0 2 * * *'

env:
  NODE_ENV: test

jobs:
  performance-baseline:
    name: Performance Baseline & Regression Detection
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Fetch full history for comparison
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'
          cache-dependency-path: server/package-lock.json
      
      - name: Install dependencies
        run: |
          cd server
          npm ci --prefer-offline --no-audit
      
      - name: Build project
        run: |
          cd server
          npm run build
      
      - name: Create performance test directory
        run: |
          mkdir -p performance-results
      
      - name: Run Build Performance Test
        run: |
          cd server
          echo "üîç Testing build performance..."
          
          # Clean build test
          rm -rf dist
          
          start_time=$(date +%s%3N)
          npm run build
          end_time=$(date +%s%3N)
          
          build_duration=$((end_time - start_time))
          echo "Build duration: ${build_duration}ms"
          echo "build_duration_ms=${build_duration}" >> ../performance-results/build.txt
          
          # Validate build artifacts
          if [ ! -d "dist" ] || [ ! -f "dist/index.js" ]; then
            echo "‚ùå Build artifacts missing"
            exit 1
          fi
          
          # Check build size
          build_size=$(du -s dist | cut -f1)
          echo "Build size: ${build_size}KB"
          echo "build_size_kb=${build_size}" >> ../performance-results/build.txt
          
          echo "‚úÖ Build performance test completed"
      
      - name: Run TypeScript Compilation Performance Test
        run: |
          cd server
          echo "üîç Testing TypeScript compilation performance..."
          
          # Clean compilation test
          rm -rf dist
          
          start_time=$(date +%s%3N)
          npm run typecheck
          end_time=$(date +%s%3N)
          
          typecheck_duration=$((end_time - start_time))
          echo "TypeScript check duration: ${typecheck_duration}ms"
          echo "typecheck_duration_ms=${typecheck_duration}" >> ../performance-results/typescript.txt
          
          echo "‚úÖ TypeScript compilation performance test completed"
      
      - name: Run Server Startup Performance Test
        run: |
          cd server
          echo "üîç Testing server startup performance..."
          
          node -e "
            async function startupPerformanceTest() {
              // Use dynamic imports for ES modules
              const { ApplicationOrchestrator } = await import('./dist/orchestration/index.js');
              const { MockLogger } = await import('./dist/utils/index.js');
              const fs = require('fs');
              const results = {};
              const logger = new MockLogger();
              
              // Test 1: Configuration loading
              const configStart = Date.now();
              const orchestrator = new ApplicationOrchestrator(logger);
              await orchestrator.loadConfiguration();
              results.config_load_ms = Date.now() - configStart;
              
              // Test 2: Prompts data loading
              const promptsStart = Date.now();
              await orchestrator.loadPromptsData();
              results.prompts_load_ms = Date.now() - promptsStart;
              results.prompts_count = orchestrator.promptsData ? orchestrator.promptsData.length : 0;
              
              // Test 3: Module initialization
              const modulesStart = Date.now();
              await orchestrator.initializeModules();
              results.modules_init_ms = Date.now() - modulesStart;
              
              // Test 4: Total startup time
              results.total_startup_ms = results.config_load_ms + results.prompts_load_ms + results.modules_init_ms;
              
              // Memory usage
              const memUsage = process.memoryUsage();
              results.memory_heap_mb = Math.round(memUsage.heapUsed / 1024 / 1024 * 100) / 100;
              results.memory_rss_mb = Math.round(memUsage.rss / 1024 / 1024 * 100) / 100;
              
              console.log('üìä Startup Performance Results:');
              console.log('   Config loading:', results.config_load_ms + 'ms');
              console.log('   Prompts loading:', results.prompts_load_ms + 'ms');
              console.log('   Modules init:', results.modules_init_ms + 'ms');
              console.log('   Total startup:', results.total_startup_ms + 'ms');
              console.log('   Prompts loaded:', results.prompts_count);
              console.log('   Memory (heap):', results.memory_heap_mb + 'MB');
              console.log('   Memory (RSS):', results.memory_rss_mb + 'MB');
              
              // Save results
              const resultsText = Object.entries(results)
                .map(([key, value]) => \`\${key}=\${value}\`)
                .join('\n');
              fs.writeFileSync('../performance-results/startup.txt', resultsText);
              
              console.log('‚úÖ Server startup performance test completed');
            }
            
            startupPerformanceTest().catch(error => {
              console.error('‚ùå Startup performance test failed:', error);
              process.exit(1);
            });
          "
      
      - name: Run CAGEERF Framework Performance Test
        run: |
          cd server
          echo "üîç Testing CAGEERF framework performance..."
          
          node -e "
            async function runCageerfPerformanceTest() {
              // Use dynamic imports for ES modules
              const { CAGEERFAnalyzer } = await import('./dist/utils/cageerf-analyzer.js');
              const { TemplateGenerator } = await import('./dist/utils/template-generator.js');
              const { SemanticAnalyzer } = await import('./dist/utils/semanticAnalyzer.js');
              const { MockLogger } = await import('./dist/utils/index.js');
              const fs = require('fs');
              
              const results = {};
            
            // Test prompts of varying complexity
            const testPrompts = [
              'Simple test prompt',
              'Create a comprehensive analysis framework that establishes clear context, performs systematic analysis, defines achievable goals, executes validation procedures, evaluates results thoroughly, and refines methodology based on feedback.',
              'Design a multi-phase project management system incorporating stakeholder analysis, risk assessment, resource optimization, timeline development, quality assurance protocols, performance monitoring, and continuous improvement mechanisms within an agile framework.'
            ];
            
            console.log('üìä CAGEERF Performance Testing:');
            
            // Test 1: CAGEERF Analysis Performance
            const analyzer = new CAGEERFAnalyzer();
            const analysisTimes = [];
            
            for (let i = 0; i < testPrompts.length; i++) {
              const start = Date.now();
              const analysis = analyzer.analyzePrompt(testPrompts[i]);
              const duration = Date.now() - start;
              analysisTimes.push(duration);
              console.log(\`   Analysis \${i + 1}: \${duration}ms (score: \${analysis.frameworkScore.toFixed(3)})\`);
            }
            
            results.cageerf_analysis_avg_ms = Math.round(analysisTimes.reduce((a, b) => a + b) / analysisTimes.length);
            results.cageerf_analysis_max_ms = Math.max(...analysisTimes);
            
            // Test 2: Template Generation Performance
            const generator = new TemplateGenerator();
            const generationTimes = [];
            const complexities = ['simple', 'intermediate', 'advanced'];
            
            for (const complexity of complexities) {
              const start = Date.now();
              const template = generator.generateTemplate({
                useCase: 'Performance Test',
                domain: 'Testing',
                complexity: complexity,
                frameworkEmphasis: {
                  context: true, analysis: true, goals: true,
                  execution: true, evaluation: true, refinement: true, framework: true
                },
                templateStyle: 'structured'
              });
              const duration = Date.now() - start;
              generationTimes.push(duration);
              console.log(\`   Generation (\${complexity}): \${duration}ms (length: \${template.content.length})\`);
            }
            
            results.template_generation_avg_ms = Math.round(generationTimes.reduce((a, b) => a + b) / generationTimes.length);
            results.template_generation_max_ms = Math.max(...generationTimes);
            
            // Test 3: Semantic Analysis Performance
            const semanticAnalyzer = new SemanticAnalyzer(new MockLogger());
            const semanticTimes = [];
            
            for (let i = 0; i < testPrompts.length; i++) {
              const testPrompt = {
                id: \`perf-test-\${i}\`,
                name: \`Performance Test \${i + 1}\`,
                content: testPrompts[i],
                description: 'Performance test prompt'
              };
              
              const start = Date.now();
              const classification = semanticAnalyzer.classifyPrompt(testPrompt);
              const duration = Date.now() - start;
              semanticTimes.push(duration);
              console.log(\`   Semantic \${i + 1}: \${duration}ms (confidence: \${classification.confidence.toFixed(3)})\`);
            }
            
            results.semantic_analysis_avg_ms = Math.round(semanticTimes.reduce((a, b) => a + b) / semanticTimes.length);
            results.semantic_analysis_max_ms = Math.max(...semanticTimes);
            
            // Save results
            const resultsText = Object.entries(results)
              .map(([key, value]) => \`\${key}=\${value}\`)
              .join('\n');
            fs.writeFileSync('../performance-results/cageerf.txt', resultsText);
            
            console.log('‚úÖ CAGEERF framework performance test completed');
            }
            
            runCageerfPerformanceTest().catch(error => {
              console.error('‚ùå CAGEERF performance test failed:', error);
              process.exit(1);
            });
          "
      
      - name: Run Memory Usage Test
        run: |
          cd server
          echo "üîç Testing memory usage patterns..."
          
          node -e "
            async function memoryTest() {
              // Use dynamic imports for ES modules
              const { ApplicationOrchestrator } = await import('./dist/orchestration/index.js');
              const { CAGEERFAnalyzer } = await import('./dist/utils/cageerf-analyzer.js');
              const { TemplateGenerator } = await import('./dist/utils/template-generator.js');
              const { MockLogger } = await import('./dist/utils/index.js');
              const fs = require('fs');
              const results = {};
              
              // Baseline memory
              global.gc && global.gc();
              const baseline = process.memoryUsage();
              
              // Test 1: Server initialization memory
              const orchestrator = new ApplicationOrchestrator(new MockLogger());
              await orchestrator.loadConfiguration();
              await orchestrator.loadPromptsData();
              await orchestrator.initializeModules();
              
              global.gc && global.gc();
              const afterInit = process.memoryUsage();
              results.init_memory_mb = Math.round((afterInit.heapUsed - baseline.heapUsed) / 1024 / 1024 * 100) / 100;
              
              // Test 2: CAGEERF operations memory
              const analyzer = new CAGEERFAnalyzer();
              const generator = new TemplateGenerator();
              
              const beforeCageerf = process.memoryUsage();
              
              // Perform multiple operations
              for (let i = 0; i < 10; i++) {
                analyzer.analyzePrompt('Test prompt for memory analysis with multiple components and framework validation');
                generator.generateTemplate({
                  useCase: 'Memory Test',
                  domain: 'Testing',
                  complexity: 'intermediate',
                  frameworkEmphasis: { context: true, analysis: true, goals: true, execution: true, evaluation: true, refinement: true, framework: true },
                  templateStyle: 'structured'
                });
              }
              
              global.gc && global.gc();
              const afterCageerf = process.memoryUsage();
              results.cageerf_memory_mb = Math.round((afterCageerf.heapUsed - beforeCageerf.heapUsed) / 1024 / 1024 * 100) / 100;
              
              // Current total memory usage
              results.total_heap_mb = Math.round(afterCageerf.heapUsed / 1024 / 1024 * 100) / 100;
              results.total_rss_mb = Math.round(afterCageerf.rss / 1024 / 1024 * 100) / 100;
              
              console.log('üìä Memory Usage Results:');
              console.log('   Initialization memory:', results.init_memory_mb + 'MB');
              console.log('   CAGEERF operations memory:', results.cageerf_memory_mb + 'MB');
              console.log('   Total heap usage:', results.total_heap_mb + 'MB');
              console.log('   Total RSS usage:', results.total_rss_mb + 'MB');
              
              // Save results
              const resultsText = Object.entries(results)
                .map(([key, value]) => \`\${key}=\${value}\`)
                .join('\n');
              fs.writeFileSync('../performance-results/memory.txt', resultsText);
              
              console.log('‚úÖ Memory usage test completed');
            }
            
            memoryTest().catch(error => {
              console.error('‚ùå Memory test failed:', error);
              process.exit(1);
            });
          " --expose-gc
      
      - name: Analyze Performance Results
        run: |
          echo "üìä Performance Analysis Summary"
          echo "================================"
          
          # Display all results
          echo "üî® Build Performance:"
          cat performance-results/build.txt
          echo ""
          
          echo "üìù TypeScript Performance:"
          cat performance-results/typescript.txt
          echo ""
          
          echo "üöÄ Startup Performance:"
          cat performance-results/startup.txt
          echo ""
          
          echo "üß† CAGEERF Performance:"
          cat performance-results/cageerf.txt
          echo ""
          
          echo "üíæ Memory Usage:"
          cat performance-results/memory.txt
          echo ""
          
          # Performance thresholds and warnings
          echo "üéØ Performance Threshold Analysis:"
          
          build_time=$(grep "build_duration_ms" performance-results/build.txt | cut -d'=' -f2)
          if [ "$build_time" -gt 10000 ]; then
            echo "‚ö†Ô∏è  Build time warning: ${build_time}ms (threshold: 10000ms)"
          else
            echo "‚úÖ Build time acceptable: ${build_time}ms"
          fi
          
          startup_time=$(grep "total_startup_ms" performance-results/startup.txt | cut -d'=' -f2)
          if [ "$startup_time" -gt 3000 ]; then
            echo "‚ö†Ô∏è  Startup time warning: ${startup_time}ms (threshold: 3000ms)"
          else
            echo "‚úÖ Startup time acceptable: ${startup_time}ms"
          fi
          
          memory_heap=$(grep "total_heap_mb" performance-results/memory.txt | cut -d'=' -f2 | cut -d'.' -f1)
          if [ "$memory_heap" -gt 150 ]; then
            echo "‚ö†Ô∏è  Memory usage warning: ${memory_heap}MB (threshold: 150MB)"
          else
            echo "‚úÖ Memory usage acceptable: ${memory_heap}MB"
          fi
          
          echo ""
          echo "‚úÖ Performance monitoring completed successfully"
      
      - name: Upload Performance Results
        uses: actions/upload-artifact@v4
        with:
          name: performance-results-${{ github.sha }}
          path: performance-results/
          retention-days: 30
      
      - name: Comment Performance Results on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            // Read performance results
            const buildResults = fs.readFileSync('performance-results/build.txt', 'utf8');
            const startupResults = fs.readFileSync('performance-results/startup.txt', 'utf8');
            const cageerfResults = fs.readFileSync('performance-results/cageerf.txt', 'utf8');
            const memoryResults = fs.readFileSync('performance-results/memory.txt', 'utf8');
            
            // Parse key metrics
            const parseResults = (text) => {
              const results = {};
              text.split('\n').forEach(line => {
                const [key, value] = line.split('=');
                if (key && value) results[key] = value;
              });
              return results;
            };
            
            const build = parseResults(buildResults);
            const startup = parseResults(startupResults);
            const cageerf = parseResults(cageerfResults);
            const memory = parseResults(memoryResults);
            
            const comment = `## üìä Performance Test Results
            
            ### üî® Build Performance
            - **Build Time**: ${build.build_duration_ms}ms
            - **Build Size**: ${build.build_size_kb}KB
            
            ### üöÄ Startup Performance
            - **Total Startup**: ${startup.total_startup_ms}ms
            - **Config Loading**: ${startup.config_load_ms}ms
            - **Prompts Loading**: ${startup.prompts_load_ms}ms
            - **Modules Init**: ${startup.modules_init_ms}ms
            - **Prompts Loaded**: ${startup.prompts_count}
            
            ### üß† CAGEERF Framework Performance
            - **Analysis (avg)**: ${cageerf.cageerf_analysis_avg_ms}ms
            - **Analysis (max)**: ${cageerf.cageerf_analysis_max_ms}ms
            - **Template Gen (avg)**: ${cageerf.template_generation_avg_ms}ms
            - **Template Gen (max)**: ${cageerf.template_generation_max_ms}ms
            - **Semantic Analysis (avg)**: ${cageerf.semantic_analysis_avg_ms}ms
            
            ### üíæ Memory Usage
            - **Total Heap**: ${memory.total_heap_mb}MB
            - **Total RSS**: ${memory.total_rss_mb}MB
            - **Initialization**: ${memory.init_memory_mb}MB
            - **CAGEERF Operations**: ${memory.cageerf_memory_mb}MB
            
            ### üéØ Performance Status
            ${parseInt(build.build_duration_ms) > 10000 ? '‚ö†Ô∏è' : '‚úÖ'} Build time: ${build.build_duration_ms}ms
            ${parseInt(startup.total_startup_ms) > 3000 ? '‚ö†Ô∏è' : '‚úÖ'} Startup time: ${startup.total_startup_ms}ms
            ${parseInt(memory.total_heap_mb) > 150 ? '‚ö†Ô∏è' : '‚úÖ'} Memory usage: ${memory.total_heap_mb}MB
            
            *Performance results are automatically generated for each PR*`;
            
            // Post comment on PR
            await github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });